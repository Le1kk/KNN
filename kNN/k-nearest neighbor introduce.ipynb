{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k近邻法(k-nearest neighbors)是由Cover和Hart于1968年提出的，它是懒惰学习(lazy learning)的著名代表。<br>它的工作机制比较简单：\n",
    "- 给定一个测试样本\n",
    "- 计算它到训练样本的距离\n",
    "- 取离测试样本最近的`k`个训练样本\n",
    "- “投票法”选出在这k个样本中出现最多的类别，就是预测的结果\n",
    "\n",
    "> 距离衡量的标准有很多，常见的有：$L_p$距离、切比雪夫距离、马氏距离、巴氏距离、余弦值等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "什么意思呢？先来看这张图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![2个类](img\\P1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们对应上面的流程来说\n",
    "- 1.给定了红色和蓝色的训练样本，绿色为测试样本\n",
    "- 2.计算绿色点到其他点的距离\n",
    "- 3.选取离绿点最近的k个点\n",
    "- 4.选取k个点中，同种颜色最多的类。例如：k=1时，k个点全是蓝色，那预测结果就是Class 1；k=3时，k个点中两个红色一个蓝色，那预测结果就是Class 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 举例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里用`欧氏距离`作为距离的衡量标准，用鸢尾花数据集举例说明。\n",
    "鸢尾花数据集有三个类别，每个类有150个样本，每个样本有4个特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先来回顾一下欧氏距离的定义(摘自维基百科)：<br>\n",
    "> 在欧几里得空间中，点 x = (x1,...,xn) 和 y = (y1,...,yn) 之间的欧氏距离为<br>\n",
    " $d(x,y):={\\sqrt  {(x_{1}-y_{1})^{2}+(x_{2}-y_{2})^{2}+\\cdots +(x_{n}-y_{n})^{2}}}={\\sqrt  {\\sum _{{i=1}}^{n}(x_{i}-y_{i})^{2}}}$<br>\n",
    " \n",
    "> 向量 ${\\displaystyle {\\vec {x}}}$的自然长度，即该点到原点的距离为<br>\n",
    " $\\|{\\vec  {x}}\\|_{2}={\\sqrt  {|x_{1}|^{2}+\\cdots +|x_{n}|^{2}}}$<br>\n",
    " 它是一个纯数值。在欧几里得度量下，两点之间线段最短。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在给出六个训练样本，分为三类，每个样本有4个特征，编号为7的`名称`是我们要预测的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|编号|花萼长度(cm)|花萼宽度(cm)|花瓣长度(cm)|花瓣宽度(cm)|名称|\n",
    "|----|----------|-----------|-----------|----------|----|\n",
    "|1|4.9|3.1|1.5|0.1|Iris setosa|\n",
    "|2|5.4|3.7|1.5|0.2|Iris setosa|\n",
    "|3|5.2|2.7|3.9|1.4|Iris versicolor|\n",
    "|4|5.0|2.0|3.5|1.0|Iris versicolor|\n",
    "|5|6.3|2.7|4.9|1.8|Iris virginica|\n",
    "|6|6.7|3.3|5.7|2.1|Iris virginica|\n",
    "|7|5.5|2.5|4.0|1.3|    ?    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照之前说的步骤，我们来计算测试样本到各个训练样本的距离。例如到第一个样本的距离$d_{1}=\\sqrt{(4.9 - 5.5)^2 + (3.1 - 2.5)^2 + (1.5 - 4.0)^2 + (0.1 - 1.3)^2} = 2.9$\n",
    "\n",
    "写一个函数来执行这个操作吧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance to 1: 2.9\n",
      "Distance to 2: 2.98496231132\n",
      "Distance to 3: 0.387298334621\n",
      "Distance to 4: 0.916515138991\n",
      "Distance to 5: 1.31909059583\n",
      "Distance to 6: 2.36854385647\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def calc_distance(iA,iB):\n",
    "    temp = np.subtract(iA, iB)      # 对应元素相减\n",
    "    temp = np.power(temp, 2)        # 元素分别平方\n",
    "    distance = np.sqrt(temp.sum())  # 先求和再开方\n",
    "    return distance\n",
    "\n",
    "testSample = np.array([5.5, 2.5, 4.0, 1.3])\n",
    "print(\"Distance to 1:\", calc_distance(np.array([4.9, 3.1, 1.5, 0.1]), testSample))\n",
    "print(\"Distance to 2:\", calc_distance(np.array([5.4, 3.7, 1.5, 0.2]), testSample))\n",
    "print(\"Distance to 3:\", calc_distance(np.array([5.2, 2.7, 3.9, 1.4]), testSample))\n",
    "print(\"Distance to 4:\", calc_distance(np.array([5.0, 2.0, 3.5, 1.0]), testSample))\n",
    "print(\"Distance to 5:\", calc_distance(np.array([6.3, 2.7, 4.9, 1.8]), testSample))\n",
    "print(\"Distance to 6:\", calc_distance(np.array([6.7, 3.3, 5.7, 2.1]), testSample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们把k定为3，那么离测试样本最近3个依次是：\n",
    "\n",
    "|编号|名称|\n",
    "|----|---|\n",
    "|3|Iris versicolor|\n",
    "|4|Iris versicolor|\n",
    "|5|Iris virginica|\n",
    "\n",
    "显然测试样本属于`Iris versicolor`类的“票数”多一点，事实上它的确属于这个类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优/缺点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里参考了[CSDN芦金宇博客上的总结](https://blog.csdn.net/ch1209498273/article/details/78440276)\n",
    "\n",
    "**优点**\n",
    "- 简单好用，容易理解，精度高，理论成熟，既可以用来做分类也可以用来做回归；\n",
    "- 可用于数值型数据和离散型数据；\n",
    "- 训练时间复杂度为O(n)；无数据输入假定；\n",
    "- 对异常值不敏感。\n",
    "\n",
    "\n",
    "**缺点**\n",
    "- 计算复杂性高；空间复杂性高；\n",
    "- 样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）；\n",
    "- 一般数值很大的时候不用这个，计算量太大。但是单个样本又不能太少，否则容易发生误分。\n",
    "- 最大的缺点是无法给出数据的内在含义。\n",
    "\n",
    "补充一点：由于它属于懒惰学习，因此需要大量的空间来存储训练实例，在预测时它还需要与已知所有实例进行比较，增大了计算量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**这里介绍一下，当样本不平衡时的影响。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![不平衡](img\\P2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从直观上可以看出X应该属于$\\omega_{1}$，这是理所应当的。对于Y看起来应该属于$\\omega_{1}$，但事实上在k范围内，更多的点属于$\\omega_{2}$，这就造成了错误分类。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一个结论"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在周志华编著的《机器学习》中证明了最近邻学习器的泛化错误率不超过贝叶斯最优分类器的错误率的两倍，在原书的226页，这里就不摘抄了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
